---
license: apache-2.0
---
# Megrez-3B: è½¯ç¡¬ååŒé‡Šæ”¾æ— ç©¹ç«¯ä¾§æ™ºèƒ½
<p align="center">
    <img src="assets/megrez_logo.png" width="400"/>
<p>
<p align="center">
        ğŸ¤— <a href="https://huggingface.co/Infinigence/Megrez-3B-Instruct">HuggingFace</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://www.modelscope.cn/models/InfiniAI/Megrez-3b-Instruct">ModelScope</a>&nbsp&nbsp | &nbsp&nbspğŸ§™ <a href="https://modelers.cn/models/INFINIGENCE-AI/Megrez-3B-Instruct">Modelers</a>&nbsp&nbsp <br> &nbsp&nbspğŸ  <a href="https://cloud.infini-ai.com/genstudio/model/mo-c73owqiotql7lozr">Infini-AI mass</a>&nbsp&nbsp | &nbsp&nbspğŸ“– <a href="https://cloud.infini-ai.com/assets/png/wechat_official_account.1f7e61401727063822266.png">WeChat Official</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href="https://cloud.infini-ai.com/assets/png/wechat_community.7dbbc0b51727063822266.png">WeChat Groups</a>&nbsp&nbsp   
</p>
<h4 align="center">
    <p>
        <b>ä¸­æ–‡</b> | <a href="https://github.com/infinigence/Infini-Megrez/blob/main/README.md">English</a>
    <p>
</h4>

# ç›®å½•

- [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
- [Megrez-3B](#megrez-3b)
  - [è¯„æµ‹ç»“æœ](#è¯„æµ‹ç»“æœ)
    - [ç»¼åˆèƒ½åŠ›](#ç»¼åˆè¯„æµ‹)
    - [å¯¹è¯èƒ½åŠ›](#å¯¹è¯èƒ½åŠ›)
    - [LLM Leaderboard](#llm-leaderboard)
    - [é•¿æ–‡æœ¬èƒ½åŠ›](#é•¿æ–‡æœ¬èƒ½åŠ›)
  - [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)
    - [HuggingFace Transformers](#ğŸ¤—-huggingface-transformers)
    - [ModelScope](#ğŸ¤–-modelscope)
    - [vLLM](#ğŸ’»-vllm)
  - [æ¨¡å‹åº”ç”¨](#æ¨¡å‹åº”ç”¨)
    - [Websearch](#websearch)
    - [ç»ˆç«¯æ¨ç†](#ç»ˆç«¯æ¨ç†)
- [Megrez-3B-Omni](#megrez-3b-omni)
  - [è¯„æµ‹ç»“æœ](#è¯„æµ‹ç»“æœ)
    - [å›¾ç‰‡ç†è§£èƒ½åŠ›](#å›¾ç‰‡ç†è§£èƒ½åŠ›)
    - [æ–‡æœ¬å¤„ç†èƒ½åŠ›](#æ–‡æœ¬å¤„ç†èƒ½åŠ›)
    - [è¯­éŸ³ç†è§£èƒ½åŠ›](#è¯­éŸ³ç†è§£èƒ½åŠ›)
    - [é€Ÿåº¦](#é€Ÿåº¦)
  - [å¿«é€Ÿä¸Šæ‰‹](#å¿«é€Ÿä¸Šæ‰‹)
    - [åœ¨çº¿ä½“éªŒ](#åœ¨çº¿ä½“éªŒ)
    - [æœ¬åœ°éƒ¨ç½²](#æœ¬åœ°éƒ¨ç½²)
    - [æ³¨æ„äº‹é¡¹](#æ³¨æ„äº‹é¡¹)
- [å¼€æºåè®®åŠä½¿ç”¨å£°æ˜](#å¼€æºåè®®åŠä½¿ç”¨å£°æ˜)

# æ¨¡å‹ä¸‹è½½

| HuggingFace                                                  | ModelScope                  |
| :----------------------------------------------------------- | --------------------------- |
| [Megrez-3B-Instruct](https://huggingface.co/Infinigence/Megrez-3B-Instruct) | [Megrez-3B-Instruct]()      |
| [Megrez-3B-Instruct-Omni](https://huggingface.co/Infinigence/Megrez-3B-Omni) | [Megrez-3B-Instruct-Omni]() |


# Megrez-3B

Megrez-3B-Instructæ˜¯ç”±æ— é—®èŠ¯ç©¹ï¼ˆ[Infinigence AI](https://cloud.infini-ai.com/platform/ai)ï¼‰å®Œå…¨è‡ªä¸»è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ã€‚Megrez-3Bæ—¨åœ¨é€šè¿‡è½¯ç¡¬ååŒç†å¿µï¼Œæ‰“é€ ä¸€æ¬¾æé€Ÿæ¨ç†ã€å°å·§ç²¾æ‚ã€ææ˜“ä¸Šæ‰‹çš„ç«¯ä¾§æ™ºèƒ½è§£å†³æ–¹æ¡ˆã€‚Megrez-3Bå…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼š

- é«˜ç²¾åº¦ï¼šMegrez-3Bè™½ç„¶å‚æ•°è§„æ¨¡åªæœ‰3Bï¼Œä½†é€šè¿‡æå‡æ•°æ®è´¨é‡ï¼ŒæˆåŠŸå¼¥åˆæ¨¡å‹èƒ½åŠ›ä»£å·®ï¼Œå°†ä¸Šä¸€ä»£14Bæ¨¡å‹çš„èƒ½åŠ›æˆåŠŸå‹ç¼©è¿›3Bå¤§å°çš„æ¨¡å‹ï¼Œåœ¨ä¸»æµæ¦œå•ä¸Šå–å¾—äº†ä¼˜ç§€çš„æ€§èƒ½è¡¨ç°ã€‚
- é«˜é€Ÿåº¦ï¼šæ¨¡å‹å°â‰ é€Ÿåº¦å¿«ã€‚Megrez-3Bé€šè¿‡è½¯ç¡¬ååŒä¼˜åŒ–ï¼Œç¡®ä¿äº†å„ç»“æ„å‚æ•°ä¸ä¸»æµç¡¬ä»¶é«˜åº¦é€‚é…ï¼Œæ¨ç†é€Ÿåº¦é¢†å…ˆåŒç²¾åº¦æ¨¡å‹æœ€å¤§300%ã€‚
- ç®€å•æ˜“ç”¨ï¼šæ¨¡å‹è®¾è®¡ä¹‹åˆæˆ‘ä»¬è¿›è¡Œäº†æ¿€çƒˆçš„è®¨è®ºï¼šåº”è¯¥åœ¨ç»“æ„è®¾è®¡ä¸Šç•™å‡ºæ›´å¤šè½¯ç¡¬ååŒçš„ç©ºé—´ï¼ˆå¦‚ReLUã€ç¨€ç–åŒ–ã€æ›´ç²¾ç®€çš„ç»“æ„ç­‰ï¼‰ï¼Œè¿˜æ˜¯ä½¿ç”¨ç»å…¸ç»“æ„ä¾¿äºå¼€å‘è€…ç›´æ¥ç”¨èµ·æ¥ï¼Ÿæˆ‘ä»¬é€‰æ‹©äº†åè€…ï¼Œå³é‡‡ç”¨æœ€åŸå§‹çš„LLaMAç»“æ„ï¼Œå¼€å‘è€…æ— éœ€ä»»ä½•ä¿®æ”¹ä¾¿å¯å°†æ¨¡å‹éƒ¨ç½²äºå„ç§å¹³å°ï¼Œæœ€å°åŒ–äºŒæ¬¡å¼€å‘å¤æ‚åº¦ã€‚
- ä¸°å¯Œåº”ç”¨ï¼šæˆ‘ä»¬æä¾›äº†å®Œæ•´çš„WebSearchæ–¹æ¡ˆã€‚æˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œäº†é’ˆå¯¹æ€§è®­ç»ƒï¼Œä½¿æ¨¡å‹å¯ä»¥è‡ªåŠ¨å†³ç­–æœç´¢è°ƒç”¨æ—¶æœºï¼Œåœ¨æœç´¢å’Œå¯¹è¯ä¸­è‡ªåŠ¨åˆ‡æ¢ï¼Œå¹¶æä¾›æ›´å¥½çš„æ€»ç»“æ•ˆæœã€‚æˆ‘ä»¬æä¾›äº†å®Œæ•´çš„éƒ¨ç½²å·¥ç¨‹ä»£ç  [github](https://github.com/infinigence/InfiniWebSearch)ï¼Œç”¨æˆ·å¯ä»¥åŸºäºè¯¥åŠŸèƒ½æ„å»ºå±äºè‡ªå·±çš„Kimiæˆ–Perplexityï¼Œå…‹æœå°æ¨¡å‹å¸¸è§çš„å¹»è§‰é—®é¢˜å’ŒçŸ¥è¯†å‚¨å¤‡ä¸è¶³çš„å±€é™ã€‚

## è¯„æµ‹ç»“æœ
### ç»¼åˆèƒ½åŠ›
|         æ¨¡å‹        | æŒ‡ä»¤æ¨¡å‹ |  Non-Emb Params | æ¨ç†é€Ÿåº¦ (tokens/s) | C-EVAL | CMMLU | MMLU | MMLU-Pro | HumanEval |  MBPP | GSM8K |  MATH |
|:---------------------:|:--------:|:---------------:|:-------------------:|:------:|:-----:|:-----:|:--------:|:---------:|:-----:|:-----:|:-----:|
| Megrez-3B-Instruct    |     Y    |       2.3       |       2329.4        |  84.8  | 74.7  | 72.8  |   46.1   |   78.7    | 71.0  | 65.5  | 28.3  |
| Qwen2-1.5B            |          |       1.3       |       3299.5        |  70.6  | 70.3  | 56.5  |   21.8   |   31.1    | 37.4  | 58.5  | 21.7  |
| Qwen2.5-1.5B          |          |       1.3       |       3318.8        |    -   |   -   | 60.9  |   28.5   |   37.2    | 60.2  | 68.5  | 35.0  |
| MiniCPM-2B            |          |       2.4       |       1930.8        |  51.1  | 51.1  | 53.5  |     -    |   50.0    | 47.3  | 53.8  | 10.2  |
| Qwen2.5-3B            |          |       2.8       |       2248.3        |    -   |   -   | 65.6  |   34.6   |   42.1    | 57.1  | 79.1  | 42.6  |
| Qwen2.5-3B-Instruct   |     Y    |       2.8       |       2248.3        |    -   |   -   |   -   |   43.7   |   74.4    | 72.7  | 86.7  | 65.9  |
| Qwen1.5-4B            |          |       3.2       |       1837.9        |  67.6  | 66.7  | 56.1  |     -    |   25.6    | 29.2  | 57.0  | 10.0  |
| Phi-3.5-mini-instruct |     Y    |       3.6       |       1559.1        |  46.1  | 46.9  | 69.0  |     -    |   62.8    | 69.6  | 86.2  | 48.5  |
| MiniCPM3-4B           |     Y    |       3.9       |        901.1        |  73.6  | 73.3  | 67.2  |     -    |   74.4    | 72.5  | 81.1  | 46.6  |
| Yi-1.5-6B             |          |       5.5       |       1542.7        |    -   | 70.8  | 63.5  |     -    |   36.5    | 56.8  | 62.2  | 28.4  |
| Qwen1.5-7B            |          |       6.5       |       1282.3        |  74.1  | 73.1  | 61.0  |   29.9   |   36.0    | 51.6  | 62.5  | 20.3  |
| Qwen2-7B              |          |       6.5       |       1279.4        |  83.2  | 83.9  | 70.3  |   40.0   |   51.2    | 65.9  | 79.9  | 44.2  |
| Qwen2.5-7B            |          |       6.5       |       1283.4        |    -   |   -   | 74.2  |   45.0   |   57.9    | 74.9  | 85.4  | 49.8  |
| Meta-Llama-3.1-8B     |          |       7.0       |       1255.9        |    -   |   -   | 66.7  |   37.1   |     -     |   -   |   -   |   -   |
| GLM-4-9B-chat         |     Y    |       8.2       |       1076.1        |  75.6  | 71.5  | 72.4  |     -    |   71.8    |   -   | 79.6  | 50.6  |
| Baichuan2-13B-Base    |          |      12.6       |        756.7        |  58.1  | 62.0  | 59.2  |     -    |   17.1    | 30.2  | 52.8  | 10.1  |
| Qwen1.5-14B           |          |      12.6       |        735.6        |  78.7  | 77.6  | 67.6  |     -    |   37.8    | 44.0  | 70.1  | 29.2  |

- Qwen2-1.5Bæ¨¡å‹çš„æŒ‡æ ‡åœ¨å…¶è®ºæ–‡å’ŒQwen2.5æŠ¥å‘Šä¸­ç‚¹æ•°ä¸ä¸€è‡´ï¼Œå½“å‰é‡‡ç”¨åŸå§‹è®ºæ–‡ä¸­çš„ç²¾åº¦
- æµ‹é€Ÿé…ç½®è¯¦è§ <a href="https://huggingface.co/Infinigence/Megrez-3B-Instruct/blob/main/README_SPEED.md">README_SPEED.md</a>

     
   
### å¯¹è¯èƒ½åŠ›
æœ¬è¡¨åªæ‘˜å‡ºæœ‰å®˜æ–¹MT-Benchæˆ–AlignBenchç‚¹æ•°çš„æ¨¡å‹  

| æ¨¡å‹              |  Non-Emb Params   | æ¨ç†é€Ÿåº¦ (tokens/s) | MT-Bench | AlignBench (ZH) |
|:-------------------:|:--------------:|:--------------------------:|:--------:|:---------------:|
| Megrez-3B-Instruct  |      2.3       |          2329.38           |   8.76   |      6.91       |
| MiniCPM-2B-sft-bf16 |      2.4       |          1930.79           |    -     |      4.64       |
| MiniCPM-2B-dpo-bf16 |      2.4       |          1930.79           |   7.25   |        -        |
| Qwen2.5-3B-Instruct |      2.8       |          2248.33           |    -     |        -        |
|     MiniCPM3-4B     |      3.9       |           901.05           |   8.41   |      6.74       |
|   Yi-1.5-6B-Chat    |      5.5       |          1542.66           |   7.5    |       6.2       |
|   Qwen1.5-7B-Chat   |      6.5       |          1282.27           |   7.6    |       6.2       |
|    Qwen2-7B-Chat    |      6.5       |          1279.37           |   8.41   |      7.21       |
| Qwen2.5-7B-Instruct |      6.5       |          1283.37           |   8.75   |        -        |
|    GLM4-9B-Chat     |      8.2       |          1076.13           |   8.35   |      7.01       |
| Baichuan2-13B-Chat  |      12.6      |           756.71           |    -     |      5.25       |

### LLM Leaderboard

| æ¨¡å‹                | Non-Emb Params | æ¨ç†é€Ÿåº¦ (tokens/s) | IFeval strict-prompt |  BBH | ARC_C | HellaSwag | WinoGrande | TriviaQA |
| :--------------------: | :------------: | :------------------------: | :----: | :---: | :---: | :-------: | :--------: | :------: |
|   Megrez-3B-Instruct   |      2.3       |          2329.38           |  78.4  | 61.0  | 90.9  |   83.6    |    72.7    |   82.5   |
|       MiniCPM-2B       |      2.4       |          1930.79           |   -    | 36.9  | 68.0  |   68.3    |     -      |   32.5   |
|       Qwen2.5-3B       |      2.8       |          2248.33           |   -    | 56.3  | 56.5  |   74.6    |    71.1    |    -     |
|  Qwen2.5-3B-instruct   |      2.8       |          2248.33           |  58.2  |   -   |   -   |     -     |     -      |    -     |
| Phi-3.5-mini-instruct  |      3.6       |          1559.09           |   -    | 69.0  | 84.6  |   69.4    |    68.5    |    -     |
|      MiniCPM3-4B       |      3.9       |           901.05           |  68.4  | 70.2  |   -   |     -     |     -      |    -     |
|     Qwen2-7B-Chat      |      6.5       |          1279.37           |   -    | 62.6  | 60.6  |   80.7    |    77.0    |    -     |
| Meta-Llama-3.1-8B-inst |      7.0       |          1255.91           |  71.5  | 28.9  | 83.4  |     -     |     -      |    -     |

### é•¿æ–‡æœ¬èƒ½åŠ›
#### Longbench 

|                       | å•æ–‡æ¡£é—®ç­” | å¤šæ–‡æ¡£é—®ç­” | æ¦‚è¦ä»»åŠ¡ | å°‘æ ·æœ¬å­¦ä¹  | äººå·¥åˆæˆä»»åŠ¡ | ä»£ç ä»»åŠ¡  | å¹³å‡ |
|:---------------------:|:------------------:|:-----------------:|:-------------:|:-----------------:|:---------------:|:----------------:|:-------:|
| Megrez-3B-Instruct    |        39.7        |        55.5       |     24.5     |       62.5        |        68.5     |       66.7       |  52.9  |
| GPT-3.5-Turbo-16k     |        50.5        |        33.7       |     21.3     |       48.1        |       54.1      |       54.1       |  43.6  |
| ChatGLM3-6B-32k       |        51.3        |        45.7       |     23.7     |       55.1        |       56.2      |       56.2       |  48.0  |
| InternLM2-Chat-7B-SFT |        47.3        |        45.2       |     25.3     |       59.9        |       67.2      |       43.5       |  48.1  |

#### é•¿æ–‡æœ¬å¯¹è¯èƒ½åŠ› 

|                          | Longbench-Chat |
|:--------------------------:|:--------------:|
| Megrez-3B-Instruct       | 4.98           |
| Vicuna-7b-v1.5-16k       | 3.51           |
| Mistral-7B-Instruct-v0.2 | 5.84           |
| ChatGLM3-6B-128k         | 6.52           |
| GLM-4-9B-Chat            | 7.72           |


## æ¨¡å‹æ¨ç†

æ¨èå®‰è£…ç‰ˆæœ¬
```
torch==2.1.2
numpy==1.26.4
transformers==4.44.2
accelerate==0.34.2
vllm==0.6.1.post2
```

#### ğŸ¤— HuggingFace Transformers
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_path = "Infinigence/Megrez-3B-Instruct"
device = "cuda"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_romote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=device, trust_remote_code=True)

messages = [{"role": "user", "content": "How to make braised chicken in brown sauce?"}]
model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to(device)
model_outputs = model.generate(
    model_inputs,
    do_sample = True,
    max_new_tokens=2048,
    top_p=0.9,
    temperature=0.2
)

output_token_ids = [
    model_outputs[i][len(model_inputs[i]):] for i in range(len(model_inputs))
]
responses = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]
print(responses)
```

#### ğŸ¤– ModelScope
```python
import torch
from modelscope import AutoTokenizer, AutoModelForCausalLM

model_path = "Infinigence/Megrez-3B-Instruct"
device = "cuda"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_romote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=device, trust_remote_code=True)

messages = [{"role": "user", "content": "How to make braised chicken in brown sauce?"}]
model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to(device)
model_outputs = model.generate(
    model_inputs,
    do_sample = True,
    max_new_tokens=2048,
    top_p=0.9,
    temperature=0.2
)

output_token_ids = [
    model_outputs[i][len(model_inputs[i]):] for i in range(len(model_inputs))
]
responses = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]
print(responses)
```

#### ğŸ’» vLLM
```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

model_name = "Infinigence/Megrez-3B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
llm = LLM(
    model=model_name,
    trust_remote_code=True,
    tensor_parallel_size=1
)

messages = [{"role": "user", "content": "How to make braised chicken in brown sauce?"}]
input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
sampling_params = SamplingParams(top_p=0.9, temperature=0.2, max_tokens=2048)
outputs = llm.generate(prompts=input_text, sampling_params=sampling_params)

print(outputs[0].outputs[0].text)
```
## æ¨¡å‹åº”ç”¨

<div id = "websearch"></div><details><summary><b>Web-Search Agent</b></summary>
æˆ‘ä»¬æ¨¡å‹è¿›è¡Œäº†é’ˆå¯¹æ€§è®­ç»ƒï¼Œå¹¶æä¾›äº†å®Œæ•´çš„å·¥ç¨‹éƒ¨ç½²æ–¹æ¡ˆã€‚InfiniWebSearch å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

1. è‡ªåŠ¨å†³å®šè°ƒç”¨æ—¶æœºï¼šè‡ªåŠ¨å†³ç­–æœç´¢è°ƒç”¨æ—¶æœºï¼Œåœ¨æœç´¢å’Œå¯¹è¯ä¸­è‡ªåŠ¨åˆ‡æ¢ï¼Œé¿å…ä¸€ç›´è°ƒç”¨æˆ–ä¸€ç›´ä¸è°ƒç”¨
2. ä¸Šä¸‹æ–‡ç†è§£ï¼šæ ¹æ®å¤šè½®å¯¹è¯ç”Ÿæˆåˆç†çš„æœç´¢queryæˆ–å¤„ç†æœç´¢ç»“æœï¼Œæ›´å¥½çš„ç†è§£ç”¨æˆ·æ„å›¾
3. å¸¦å‚è€ƒä¿¡æ¯çš„ç»“æ„åŒ–è¾“å‡ºï¼šæ¯ä¸ªç»“è®ºæ³¨æ˜å‡ºå¤„ï¼Œä¾¿äºæŸ¥éªŒ
4.ä¸€ä¸ªæ¨¡å‹ä¸¤ç§ç”¨æ³•ï¼šé€šè¿‡sys promptåŒºåˆ†WebSearchåŠŸèƒ½å¼€å¯ä¸å¦ï¼Œå…¼é¡¾LLMçš„é«˜ç²¾åº¦ä¸WebSearchçš„ç”¨æˆ·ä½“éªŒï¼Œä¸¤ç§èƒ½åŠ›ä¸ä¹±çªœ  

æˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œäº†é’ˆå¯¹æ€§è®­ç»ƒï¼Œä½¿æ¨¡å‹å¯ä»¥è‡ªåŠ¨å†³ç­–æœç´¢è°ƒç”¨æ—¶æœºï¼Œåœ¨æœç´¢å’Œå¯¹è¯ä¸­è‡ªåŠ¨åˆ‡æ¢ï¼Œå¹¶æä¾›æ›´å¥½çš„æ€»ç»“æ•ˆæœã€‚æˆ‘ä»¬æä¾›äº†å®Œæ•´çš„éƒ¨ç½²å·¥ç¨‹ä»£ç  ï¼Œç”¨æˆ·å¯ä»¥åŸºäºè¯¥åŠŸèƒ½æ„å»ºå±äºè‡ªå·±çš„Kimiæˆ–Perplexityï¼Œå…‹æœå°æ¨¡å‹å¸¸è§çš„å¹»è§‰é—®é¢˜å’ŒçŸ¥è¯†å‚¨å¤‡ä¸è¶³çš„å±€é™ã€‚
<div align="center">
    <img src="assets/websearch_demo.gif" width="100%" alt="Example GIF">
</div>
</details>

<div id = "ç»ˆç«¯æ¨ç†"></div><details><summary><b>ç»ˆç«¯æ¨ç†</b></summary>
MLCChat æ˜¯ä¸€æ¬¾è®¾å¤‡å†…ç½®èŠå¤©åº”ç”¨ï¼Œå³å°†ä¸Šçº¿ã€‚
<div align="center">
    <img src="assets/deployment_android.gif" width="50%" alt="Example GIF">
</div>
</details>




# Megrez-3B-Omni

æˆ‘ä»¬åŒæ—¶å¼€æºäº†ç›¸åº”çš„å¤šæ¨¡æ¨¡å‹ï¼Œ**Megrez-3B-Omni**ã€‚  
Megrez-3B-Omniæ˜¯ç”±æ— é—®èŠ¯ç©¹ï¼ˆ[Infinigence AI](https://cloud.infini-ai.com/platform/ai)ï¼‰ç ”å‘çš„**ç«¯ä¾§å…¨æ¨¡æ€**ç†è§£æ¨¡å‹ï¼ŒåŸºäºæ— é—®å¤§è¯­è¨€æ¨¡å‹Megrez-3B-Instructæ‰©å±•ï¼ŒåŒæ—¶å…·å¤‡å›¾ç‰‡ã€æ–‡æœ¬ã€éŸ³é¢‘ä¸‰ç§æ¨¡æ€æ•°æ®çš„ç†è§£åˆ†æèƒ½åŠ›ï¼Œåœ¨ä¸‰ä¸ªæ–¹é¢å‡å–å¾—æœ€ä¼˜ç²¾åº¦
- åœ¨å›¾åƒç†è§£æ–¹é¢ï¼ŒåŸºäºSigLip-400Mæ„å»ºå›¾åƒTokenï¼Œåœ¨OpenCompassæ¦œå•ä¸Šï¼ˆç»¼åˆ8ä¸ªä¸»æµå¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ï¼‰å¹³å‡å¾—åˆ†66.2ï¼Œè¶…è¶ŠLLaVA-NeXT-Yi-34Bç­‰æ›´å¤§å‚æ•°è§„æ¨¡çš„æ¨¡å‹ã€‚Megrez-3B-Omniä¹Ÿæ˜¯åœ¨MMEã€MMMUã€OCRBenchç­‰æµ‹è¯•é›†ä¸Šç›®å‰ç²¾åº¦æœ€é«˜çš„å›¾åƒç†è§£æ¨¡å‹ä¹‹ä¸€ï¼Œåœ¨åœºæ™¯ç†è§£ã€OCRç­‰æ–¹é¢å…·æœ‰è‰¯å¥½è¡¨ç°ã€‚
- åœ¨è¯­è¨€ç†è§£æ–¹é¢ï¼ŒMegrez-3B-Omniå¹¶æœªç‰ºç‰²æ¨¡å‹çš„æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼Œç»¼åˆèƒ½åŠ›è¾ƒå•æ¨¡æ€ç‰ˆæœ¬ï¼ˆMegrez-3B-Instructï¼‰ç²¾åº¦å˜åŒ–å°äº2%ï¼Œä¿æŒåœ¨C-EVALã€MMLU (Proï¼‰ã€AlignBenchç­‰å¤šä¸ªæµ‹è¯•é›†ä¸Šçš„æœ€ä¼˜ç²¾åº¦ä¼˜åŠ¿ï¼Œä¾ç„¶å–å¾—è¶…è¶Šä¸Šä¸€ä»£14Bæ¨¡å‹çš„èƒ½åŠ›è¡¨ç°
- åœ¨è¯­éŸ³ç†è§£æ–¹é¢ï¼Œé‡‡ç”¨Whisper-large-v3çš„Encoderä½œä¸ºè¯­éŸ³è¾“å…¥ï¼Œæ”¯æŒä¸­è‹±æ–‡è¯­éŸ³è¾“å…¥åŠå¤šè½®å¯¹è¯ï¼Œæ”¯æŒå¯¹è¾“å…¥å›¾ç‰‡çš„è¯­éŸ³æé—®ï¼Œæ ¹æ®è¯­éŸ³æŒ‡ä»¤ç›´æ¥å“åº”æ–‡æœ¬ï¼Œåœ¨å¤šé¡¹åŸºå‡†ä»»åŠ¡ä¸Šå–å¾—äº†é¢†å…ˆçš„ç»“æœ

## è¯„æµ‹ç»“æœ
### å›¾ç‰‡ç†è§£èƒ½åŠ›

å·¦å›¾ä¸ºMegrez-3B-Omniä¸å…¶ä»–å¼€æºæ¨¡å‹åœ¨å›¾ç‰‡ç†è§£å„ä»»åŠ¡çš„èƒ½åŠ›æ¯”è¾ƒï¼›  
å³å›¾ä¸ºMegrez-3B-Omniåœ¨opencompassæµ‹è¯•é›†ä¸Šè¡¨ç°ï¼Œå‚è€ƒ [InternVL 2.5 Blog Post](https://internvl.github.io/blog/2024-12-05-InternVL-2.5/)*

 <div style="display: flex; justify-content: space-between;">
  <img src="assets/multitask.jpg" alt="Image 1" style="width: 45%;">
  <img src="assets/opencompass.jpg" alt="Image 2" style="width: 45%;">
</div>

<!-- ![Multitask](assets/multitask.jpg)

![OpencompassBmk](assets/opencompass.jpg) -->

| model                 | basemodel             | å‘å¸ƒæ—¶é—´       | OpenCompass (åœ¨çº¿) | MME      | MMMU val  | OCRBench | Math-Vista-Mini | RealWorldQA | MMVet  | hallusionBench | MMB TEST(en) | MMB TEST(zh) | TextVQA val | AI2D_TEST | MMstar    | DocVQA_TEST |
|:-----------------------:|:-----------------------:|:----------------:|:--------------------:|:----------:|:-----------:|:----------:|:-----------------:|:-------------:|:--------:|:----------------:|:--------------:|:--------------:|:-------------:|:-----------:|:-----------:|:-------------:|
| **Megrez-3B-Omni**    | **Megrez-3B**         | **2024.12.16** | **66.2**           | **2315** | **51.89** | **82.8** | **62**          | **71.89**   | **60** | **50.12**      | **80.8**     | **82.3**     | **80.3**    | **82.05** | **60.46** | **91.62**   |
| Qwen2-VL-2B-Instruct  | Qwen2-1.5B            | 2024.08.28     | 57.2               | 1872     | 41.1      | 79.4     | 43              | 62.9        | 49.5   | 41.7           | 74.9         | 73.5         | 79.7        | 74.7      | 48        | 90.1        |
| InternVL2.5-2B        | Internlm2.5-1.8B-chat | 2024.12.06     | 59.9               | 2138     | 43.6      | 80.4     | 51.3            | 60.1        | 60.8   | 42.6           | 74.7         | 71.9         | 74.3        | 74.9      | 53.7      | 88.7        |
| BlueLM-V-3B           | -                     | 2024.11.29     | 66.1               | -        | 45.1      | 82.9     | 60.8            | 66.7        | 61.8   | 48             | 83           | 80.5         | 78.4        | 85.3      | 62.3      | 87.8        |
| InternVL2.5-4B        | Qwen2.5-3B-Instruct   | 2024.12.06     | 65.1               | 2337     | 52.3      | 82.8     | 60.5            | 64.3        | 60.6   | 46.3           | 81.1         | 79.3         | 76.8        | 81.4      | 58.3      | 91.6        |
| Baichuan-Omni         | Unknown-7B            | 2024.10.11     | -                  | 2186     | 47.3      | 70.0     | 51.9            | 62.6        | 65.4   | 47.8           | 76.2         | 74.9         | 74.3        | -         | -         | -           |
| MiniCPM-V-2.6         | Qwen2-7B              | 2024.08.06     | 65.2               | 2348     | 49.8      | 85.2     | 60.6            | 69.7        | 60     | 48.1           | 81.2         | 79           | 80.1        | 82.1      | 57.26     | 90.8        |
| Qwen2-VL-7B-Instruct  | Qwen2-7B              | 2024.08.28     | 67                 | 2326     | 54.1      | 84.5     | 58.2            | 70.1        | 62     | 50.6           | 83           | 80.5         | 84.3        | 83        | 60.7      | 94.5        |
| MiniCPM-Llama3-V-2.5  | Llama3-Instruct 8B    | 2024.05.20     | 58.8               | 2024     | 45.8      | 72.5     | 54.3            | 63.5        | 52.8   | 42.4           | 77.2         | 74.2         | 76.6        | 78.4      | -         | 84.8        |
| VITA                  | Mixtral 8x7B          | 2024.08.12     | -                  | 2097     | 47.3      | 67.8     | 44.9            | 59          | 41.6   | 39.7           | 74.7         | 71.4         | 71.8        | -         | -         | -           |
| GLM-4V-9B             | GLM-4-9B              | 2024.06.04     | 59.1               | 2018     | 46.9      | 77.6     | 51.1            | -           | 58     | 46.6           | 81.1         | 79.4         | -           | 81.1      | 58.7      | -           |
| LLaVA-NeXT-Yi-34B     | Yi-34B                | 2024.01.18     | 55                 | 2006     | 48.8      | 57.4     | 40.4            | 66          | 50.7   | 34.8           | 81.1         | 79           | 69.3        | 78.9      | 51.6      | -           |
| Qwen2-VL-72B-Instruct | Qwen2-72B             | 2024.08.28     | 74.8               | 2482     | 64.5      | 87.7     | 70.5            | 77.8        | 74     | 58.1           | 86.5         | 86.6         | 85.5        | 88.1      | 68.3      | 96.5        |

### æ–‡æœ¬å¤„ç†èƒ½åŠ›

|                       |          |             |                                       | å¯¹è¯&æŒ‡ä»¤ |                 |        | ä¸­æ–‡&è‹±æ–‡ä»»åŠ¡ |            |       |          |  ä»£ç ä»»åŠ¡ |       | æ•°å­¦ä»»åŠ¡ |       |
|:---------------------:|:--------:|:-----------:|:-------------------------------------:|:---------:|:---------------:|:------:|:-------------:|:----------:|:-----:|:--------:|:---------:|:-----:|:--------:|:-----:|
|         models        | æŒ‡ä»¤æ¨¡å‹ |   å‘å¸ƒæ—¶é—´  | Transformerå‚æ•°é‡ ï¼ˆä¸å«emb&softmaxï¼‰ |  MT-Bench | AlignBench (ZH) | IFEval |  C-EVAL (ZH)  | CMMLU (ZH) | MMLU  | MMLU-Pro | HumanEval |  MBPP |   GSM8K  |  MATH |
| Megrez-3B-Omni        |     Y    |  2024.12.16 |                  2.3                  |    8.4    |       6.5       |  66.5  |     84.0      |    75.3    | 73.3  |   45.2   |   72.6    | 60.6  |   63.8   | 27.3  |
| Megrez-3B-Instruct    |     Y    |  2024.12.16 |                  2.3                  |   8.64    |      7.06       |  68.6  |     84.8      |    74.7    | 72.8  |   46.1   |   78.7    | 71.0  |   65.5   | 28.3  |
| Baichuan-Omni         |     Y    |  2024.10.11 |                  7.0                  |     -     |        -        |    -   |     68.9      |    72.2    |  65.3 |     -    |     -     |   -   |     -    |   -   |
| VITA                  |     Y    |  2024.08.12 |                 12.9                  |     -     |        -        |    -   |     56.7      |    46.6    | 71.0  |     -    |     -     |   -   |   75.7   |   -   |
| Qwen1.5-7B            |          |  2024.02.04 |                  6.5                  |     -     |        -        |    -   |     74.1      |    73.1    | 61.0  |   29.9   |   36.0    | 51.6  |   62.5   | 20.3  |
| Qwen1.5-7B-Chat       |     Y    |  2024.02.04 |                  6.5                  |   7.60    |      6.20       |    -   |     67.3      |      -     | 59.5  |   29.1   |   46.3    | 48.9  |   60.3   | 23.2  |
| Qwen1.5-14B           |          |  2024.02.04 |                 12.6                  |     -     |        -        |    -   |     78.7      |    77.6    | 67.6  |     -    |   37.8    | 44.0  |   70.1   | 29.2  |
| Qwen1.5-14B-Chat      |     Y    |  2024.02.04 |                 12.6                  |    7.9    |        -        |    -   |       -       |      -     |   -   |     -    |     -     |   -   |     -    |   -   |
| Qwen2-7B              |          |  2024.06.07 |                  6.5                  |     -     |        -        |    -   |     83.2      |    83.9    | 70.3  |   40.0   |   51.2    | 65.9  |   79.9   | 44.2  |
| Qwen2-7b-Instruct     |     Y    |  2024.06.07 |                  6.5                  |   8.41    |      7.21       |  51.4  |     80.9      |    77.2    | 70.5  |   44.1   |   79.9    | 67.2  |   85.7   | 52.9  |
| Qwen2.5-3B-Instruct   |     Y    |  2024.9.19  |                  2.8                  |     -     |        -        |    -   |       -       |      -     |   -   |   43.7   |   74.4    | 72.7  |   86.7   | 65.9  |
| Qwen2.5-7B            |          |  2024.9.19  |                  6.5                  |     -     |        -        |    -   |       -       |      -     | 74.2  |   45.0   |   57.9    | 74.9  |   85.4   | 49.8  |
| Qwen2.5-7B-Instruct   |     Y    |  2024.09.19 |                  6.5                  |   8.75    |        -        |  74.9  |       -       |      -     |   -   |   56.3   |   84.8    | 79.2  |   91.6   | 75.5  |
| Llama-3.1-8B          |          |  2024.07.23 |                  7.0                  |    8.3    |       5.7       |  71.5  |     55.2      |    55.8    | 66.7  |   37.1   |     -     |   -   |   84.5   | 51.9  |
| Llama-3.2-3B          |          |  2024.09.25 |                  2.8                  |     -     |        -        |  77.4  |       -       |      -     | 63.4  |     -    |     -     |   -   |   77.7   | 48.0  |
| Phi-3.5-mini-instruct |     Y    |  2024.08.23 |                  3.6                  |    8.6    |       5.7       |  49.4  |     46.1      |    46.9    | 69.0  |   47.4   |   62.8    | 69.6  |   86.2   | 48.5  |
| MiniCPM3-4B           |     Y    |  2024.09.05 |                  3.9                  |   8.41    |      6.74       |  68.4  |     73.6      |    73.3    | 67.2  |     -    |   74.4    | 72.5  |   81.1   | 46.6  |
| Yi-1.5-6B-Chat        |     Y    |  2024.05.11 |                  5.5                  |   7.50    |      6.20       |    -   |     74.2      |    74.7    | 61.0  |     -    |   64.0    | 70.9  |   78.9   | 40.5  |
| GLM-4-9B-chat         |     Y    |  2024.06.04 |                  8.2                  |   8.35    |      7.01       |  64.5  |     75.6      |    71.5    | 72.4  |     -    |   71.8    |   -   |   79.6   | 50.6  |
| Baichuan2-13B-Base    |          |  2023.09.06 |                 12.6                  |     -     |      5.25       |    -   |     58.1      |    62.0    | 59.2  |     -    |   17.1    | 30.2  |   52.8   | 10.1  |

æ³¨ï¼šQwen2-1.5Bæ¨¡å‹çš„æŒ‡æ ‡åœ¨è®ºæ–‡å’ŒQwen2.5æŠ¥å‘Šä¸­ç‚¹æ•°ä¸ä¸€è‡´ï¼Œå½“å‰é‡‡ç”¨åŸå§‹è®ºæ–‡ä¸­çš„ç²¾åº¦


### è¯­éŸ³ç†è§£èƒ½åŠ›

|       Model      |     Base model     | Realease Time | Fleurs test-zh | WenetSpeech test_net | WenetSpeech test_meeting |
|:----------------:|:------------------:|:-------------:|:--------------:|:--------------------:|:------------------------:|
| Whisper-large-v3 |          -         |   2023.11.06  |      12.4      |         17.5         |           30.8           |
|  Qwen2-Audio-7B  |      Qwen2-7B      |   2024.08.09  |        9       |          11          |           10.7           |
|  Baichuan2-omni  |     Unknown-7B     |   2024.10.11  |        7       |          6.9         |            8.4           |
|       VITA       |    Mixtral 8x7B    |   2024.08.12  |        -       |      -/12.2(CER)     |        -/16.5(CER)       |
|  Megrez-3B-Omni  | Megrez-3B-Instruct |   2024.12.16  |      10.8      |           -          |           16.44          |


### é€Ÿåº¦

|                | image_tokens | prefill (tokens/s) | decode (tokens/s) |
|:--------------:|:------------:|:------------------:|:-----------------:|
| Megrez-3B-Omni |      448     |       6312.66      |       1294.9      |
| Qwen2-VL-2B    |     1378     |       7349.39      |       685.66      |
| MiniCPM-V-2_6  |      448     |       2167.09      |       452.51      |

å®éªŒè®¾ç½®ï¼š 
- æµ‹è¯•ç¯å¢ƒï¼šNVIDIA H100ï¼ŒvLLMä¸‹è¾“å…¥128ä¸ªText tokenå’Œä¸€å¼ 1480x720å¤§å°å›¾ç‰‡ï¼Œè¾“å‡º128ä¸ªtokenï¼Œnum_seqså›ºå®šä¸º8
- Qwen2-VL-2Bè™½ç„¶å…¶å…·å¤‡æ›´å°å°ºå¯¸çš„åŸºåº§æ¨¡å‹ï¼Œä½†ç¼–ç ä¸Šè¿°å¤§å°å›¾ç‰‡åçš„image_tokenç›¸è¾ƒMegrez-3B-Omniå¤šå¾ˆå¤šï¼Œå¯¼è‡´æ­¤å®éªŒä¸‹çš„decodeé€Ÿåº¦å°äºMegrez-3B-Omni


## å¿«é€Ÿä¸Šæ‰‹

### åœ¨çº¿ä½“éªŒ

[HF Chat Demo](https://huggingface.co/spaces/Infinigence/Infinigence-AI-Chat-Demo)

### æœ¬åœ°éƒ¨ç½²

ç¯å¢ƒå®‰è£…å’ŒVllmæ¨ç†ä»£ç ç­‰éƒ¨ç½²é—®é¢˜å¯ä»¥å‚è€ƒ [Infini-Megrez-Omni](https://github.com/infinigence/Infini-Megrez-Omni)

å¦‚ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨transformersè¿›è¡Œæ¨ç†çš„ä¾‹å­ï¼Œé€šè¿‡åœ¨contentå­—æ®µä¸­åˆ†åˆ«ä¼ å…¥textã€imageå’Œaudioï¼Œå¯ä»¥å›¾æ–‡/å›¾éŸ³ç­‰å¤šç§æ¨¡æ€å’Œæ¨¡å‹è¿›è¡Œäº¤äº’ã€‚
```python
import torch
from transformers import AutoModelForCausalLM

path = "{{PATH_TO_PRETRAINED_MODEL}}"  # Change this to the path of the model.

model = (
    AutoModelForCausalLM.from_pretrained(
        path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
    )
    .eval()
    .cuda()
)

# Chat with text and image
messages = [
    {
        "role": "user",
        "content": {
            "text": "Please describe the content of the image.",
            "image": "./data/sample_image.jpg",
        },
    },
]

# Chat with audio and image
messages = [
    {
        "role": "user",
        "content": {
            "image": "./data/sample_image.jpg",
            "audio": "./data/sample_audio.m4a",
        },
    },
]

MAX_NEW_TOKENS = 100
response = model.chat(
    messages,
    sampling=False,
    max_new_tokens=MAX_NEW_TOKENS,
    temperature=0,
)
print(response)

```

## æ³¨æ„äº‹é¡¹
1. å›¾ç‰‡è¾“å…¥ä¸‹ï¼Œåªæ”¯æŒé¦–è½®è¾“å…¥ï¼›è¯­éŸ³å’Œæ–‡æœ¬å¯ä»¥è‡ªç”±åˆ‡æ¢
2. ASRæ‹¼æ³•
3. OCRåœºæ™¯ä¸‹æˆ‘ä»¬æ¨èå…³é—­é‡‡æ ·è¿›è¡Œæ¨ç†ï¼Œå³sampling=False


# å¼€æºåè®®åŠä½¿ç”¨å£°æ˜
- åè®®ï¼šæœ¬ä»“åº“ä¸­ä»£ç ä¾ç…§ [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0) åè®®å¼€æºã€‚
- å¹»è§‰ï¼šå¤§æ¨¡å‹å¤©ç„¶å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œç”¨æˆ·ä½¿ç”¨è¿‡ç¨‹ä¸­è¯·å‹¿å®Œå…¨ç›¸ä¿¡æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ã€‚
- ä»·å€¼è§‚åŠå®‰å…¨æ€§ï¼šæœ¬æ¨¡å‹å·²å°½å…¨åŠ›ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®çš„åˆè§„æ€§ï¼Œä½†ç”±äºæ•°æ®çš„å¤§ä½“é‡åŠå¤æ‚æ€§ï¼Œä»æœ‰å¯èƒ½å­˜åœ¨ä¸€äº›æ— æ³•é¢„è§çš„é—®é¢˜ã€‚å¦‚æœå‡ºç°ä½¿ç”¨æœ¬å¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

